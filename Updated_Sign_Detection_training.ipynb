{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b457d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "# Define the pre-trained model from TensorFlow Hub\n",
    "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_96/feature_vector/4\"\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(MODEL_URL, input_shape=(96, 96, 3), trainable=False)\n",
    "])\n",
    "\n",
    "# Add a custom head to the pre-trained model\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the training and validation data directories\n",
    "train_dir = \"Tensorflow/workspace/images/collectedImages/\"\n",
    "valid_dir = \"Tensorflow/workspace/images/collectedImages/\"\n",
    "\n",
    "# Define the image size and batch size for data preprocessing\n",
    "IMG_SIZE = (96, 96)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the hand landmark detection module from Mediapipe\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Define the function to extract hand landmarks from an image using Mediapipe\n",
    "def extract_hand_landmarks(image):\n",
    "    # Convert the image to RGB format and process it with the hand landmark detection module\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb = (image_rgb * 255).astype(np.uint8)\n",
    "    results = mp_hands.process(image_rgb)\n",
    "\n",
    "    # If hand landmarks are detected, extract them and return the coordinates\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        landmark_coords = np.zeros((21, 2))\n",
    "        for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "            landmark_coords[idx] = [landmark.x, landmark.y]\n",
    "        return landmark_coords\n",
    "\n",
    "    # If no hand landmarks are detected, return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define the function to preprocess an image by extracting the hand landmarks and resizing it\n",
    "def preprocess_image(image):\n",
    "    # Load the image and extract the hand landmarks\n",
    "    #image = cv2.imread(image_path)\n",
    "    landmarks = extract_hand_landmarks(image)\n",
    "\n",
    "    # If hand landmarks are detected, crop the image to the bounding box around the hand\n",
    "    if landmarks is not None:\n",
    "        xmin = int(np.min(landmarks[:, 0]) * image.shape[1])\n",
    "        ymin = int(np.min(landmarks[:, 1]) * image.shape[0])\n",
    "        xmax = int(np.max(landmarks[:, 0]) * image.shape[1])\n",
    "        ymax = int(np.max(landmarks[:, 1]) * image.shape[0])\n",
    "        image = image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Resize the image to the desired input size for the model\n",
    "    image = cv2.resize(image, IMG_SIZE)\n",
    "\n",
    "    # Return the preprocessed image\n",
    "    return image\n",
    "\n",
    "# Define the training and validation data generators with data augmentation\n",
    "train_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "valid_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Generate the training and validation datasets\n",
    "train_data = train_data_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_data = valid_data_gen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Fine-tune the pre-trained model on the sign language dataset\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('sign_language_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "# Define the hand landmark detection module from Mediapipe\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Define the function to extract hand landmarks from an image using Mediapipe\n",
    "def extract_hand_landmarks(image):\n",
    "    # Convert the image to RGB format and process it with the hand landmark detection module\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb = (image_rgb * 255).astype(np.uint8)\n",
    "    results = mp_hands.process(image_rgb)\n",
    "\n",
    "    # If hand landmarks are detected, extract them and return the coordinates\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        landmark_coords = np.zeros((21, 2))\n",
    "        for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "            landmark_coords[idx] = [landmark.x, landmark.y]\n",
    "        return landmark_coords\n",
    "\n",
    "    # If no hand landmarks are detected, return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define the function to preprocess an image by extracting the hand landmarks and resizing it\n",
    "def preprocess_image(image):\n",
    "    # Load the image and extract the hand landmarks\n",
    "    #image = cv2.imread(image_path)\n",
    "    landmarks = extract_hand_landmarks(image)\n",
    "\n",
    "    # If hand landmarks are detected, crop the image to the bounding box around the hand\n",
    "    if landmarks is not None:\n",
    "        xmin = int(np.min(landmarks[:, 0]) * image.shape[1])\n",
    "        ymin = int(np.min(landmarks[:, 1]) * image.shape[0])\n",
    "        xmax = int(np.max(landmarks[:, 0]) * image.shape[1])\n",
    "        ymax = int(np.max(landmarks[:, 1]) * image.shape[0])\n",
    "        image = image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Resize the image to the desired input size for the model\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "\n",
    "    # Return the preprocessed image\n",
    "    return image\n",
    "\n",
    "# Define the function to predict the hand sign from an image using the trained model\n",
    "def predict_hand_sign(image):\n",
    "    # Preprocess the image and expand the dimensions to match the input shape of the model\n",
    "    image = preprocess_image(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Use the trained model to predict the hand sign from the image\n",
    "    prediction = model.predict(image)\n",
    "\n",
    "    # Return the predicted hand sign\n",
    "    return prediction.argmax()\n",
    "\n",
    "def run_hand_sign_detection(model_path):\n",
    "    # Load the saved model\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "    # Define the hand landmark detection module from Mediapipe\n",
    "    mp_hands = mp.solutions.hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "\n",
    "    # Initialize the webcam video stream\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video stream\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Extract the hand landmarks and preprocess the image\n",
    "        landmarks = extract_hand_landmarks(frame)\n",
    "        if landmarks is not None:\n",
    "            image = preprocess_image(frame)\n",
    "\n",
    "            # Make a prediction with the trained model\n",
    "            prediction = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "            label = np.argmax(prediction)\n",
    "            confidence = prediction[label]\n",
    "            labels = {\"0\":\"hello\",\"1\":\"iloveyou\",\"2\":\"no\",\"3\":\"Thank You\",\"4\":\"yes\"}\n",
    "            label = labels[str(label)]\n",
    "            # Draw a bounding box around the detected hand and display the predicted label and confidence score\n",
    "            xmin = int(np.min(landmarks[:, 0]) * frame.shape[1])\n",
    "            ymin = int(np.min(landmarks[:, 1]) * frame.shape[0])\n",
    "            xmax = int(np.max(landmarks[:, 0]) * frame.shape[1])\n",
    "            ymax = int(np.max(landmarks[:, 1]) * frame.shape[0])\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label}: {confidence:.2f}\", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Hand Sign Detection\", frame)\n",
    "\n",
    "        # Exit if the user presses the 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "run_hand_sign_detection('sign_language_model.h5')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
